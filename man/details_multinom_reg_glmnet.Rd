% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multinom_reg_glmnet.R
\name{details_multinom_reg_glmnet}
\alias{details_multinom_reg_glmnet}
\title{Multinomial regression via glmnet}
\description{
\code{\link[glmnet:glmnet]{glmnet::glmnet()}} fits a model that uses linear predictors to predict
multiclass data using the multinomial distribution.
}
\details{
For this engine, there is a single mode: classification
\subsection{Tuning Parameters}{

This model has 2 tuning parameters:
\itemize{
\item \code{penalty}: Amount of Regularization (type: double, default: see
below)
\item \code{mixture}: Proportion of Lasso Penalty (type: double, default: 1.0)
}

A value of \code{mixture = 1} corresponds to a pure lasso model, while
\code{mixture = 0} indicates ridge regression.

The \code{penalty} parameter has no default and requires a single numeric
value. For more details about this, and the \code{glmnet} model in general,
see \link{glmnet-details}.
}

\subsection{Translation from parsnip to the original package}{\if{html}{\out{<div class="r">}}\preformatted{multinom_reg(penalty = double(1), mixture = double(1)) \%>\% 
  set_engine("glmnet") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0
##   mixture = double(1)
## 
## Computational engine: glmnet 
## 
## Model fit template:
## glmnet::glmnet(x = missing_arg(), y = missing_arg(), weights = missing_arg(), 
##     alpha = double(1), family = "multinomial")
}
}

\subsection{Preprocessing requirements}{

Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via
\code{\link[=fit.model_spec]{fit.model_spec()}}, parsnip will
convert factor columns to indicators.

Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one. By default, \code{\link[glmnet:glmnet]{glmnet::glmnet()}} uses
the argument \code{standardize = TRUE} to center and scale the data.
}

\subsection{Example}{
\subsection{Classification Example}{

We’ll model the island of the penguins with two predictors in the same
unit (mm): bill length and bill depth.\if{html}{\out{<div class="r">}}\preformatted{library(tidymodels)
tidymodels_prefer()
data(penguins)

penguins <- penguins \%>\% select(island, starts_with("bill_"))
penguins_train <- penguins[-c(21, 153, 31, 277, 1), ]
penguins_test  <- penguins[ c(21, 153, 31, 277, 1), ]
}\if{html}{\out{</div>}}

We can define the model with specific parameters:\if{html}{\out{<div class="r">}}\preformatted{mr_cls_spec <- 
  multinom_reg(penalty = 0.1) \%>\% 
  set_engine("glmnet")
mr_cls_spec
}\if{html}{\out{</div>}}\preformatted{## Multinomial Regression Model Specification (classification)
## 
## Main Arguments:
##   penalty = 0.1
## 
## Computational engine: glmnet
}

Now we create the model fit object:\if{html}{\out{<div class="r">}}\preformatted{set.seed(1)
mr_cls_fit <- mr_cls_spec \%>\% fit(island ~ ., data = penguins_train)
mr_cls_fit
}\if{html}{\out{</div>}}\preformatted{## parsnip model object
## 
## Fit time:  6ms 
## 
## Call:  glmnet::glmnet(x = maybe_matrix(x), y = y, family = "multinomial") 
## 
##    Df  \%Dev  Lambda
## 1   0  0.00 0.31730
## 2   1  3.43 0.28910
## 3   1  6.30 0.26340
## 4   1  8.74 0.24000
## 5   1 10.83 0.21870
## 6   1 12.62 0.19930
## 7   1 14.17 0.18160
## 8   1 15.51 0.16540
## 9   1 16.67 0.15070
## 10  1 17.68 0.13740
## 11  1 18.56 0.12520
## 12  2 19.93 0.11400
## 13  2 21.31 0.10390
## 14  2 22.50 0.09467
## 15  2 23.52 0.08626
## 16  2 24.40 0.07860
## 17  2 25.16 0.07162
## 18  2 25.81 0.06526
## 19  2 26.37 0.05946
## 20  2 26.86 0.05418
## 21  2 27.27 0.04936
## 22  2 27.63 0.04498
## 23  2 27.94 0.04098
## 24  2 28.21 0.03734
## 25  2 28.44 0.03402
## 26  2 28.63 0.03100
## 27  2 28.80 0.02825
## 28  2 28.94 0.02574
## 29  2 29.06 0.02345
## 30  2 29.17 0.02137
## 31  2 29.26 0.01947
## 32  2 29.33 0.01774
## 33  2 29.39 0.01616
## 34  2 29.45 0.01473
## 35  2 29.49 0.01342
## 36  2 29.53 0.01223
## 37  2 29.56 0.01114
## 38  2 29.59 0.01015
## 39  2 29.61 0.00925
## 40  2 29.63 0.00843
## 41  2 29.65 0.00768
## 42  2 29.67 0.00700
## 43  2 29.68 0.00638
## 44  2 29.69 0.00581
## 45  2 29.70 0.00529
## 46  2 29.71 0.00482
## 47  2 29.71 0.00439
## 48  2 29.72 0.00400
## 49  2 29.72 0.00365
## 50  2 29.73 0.00332
## 51  2 29.73 0.00303
## 52  2 29.74 0.00276
## 53  2 29.74 0.00251
## 54  2 29.74 0.00229
## 55  2 29.75 0.00209
## 56  2 29.75 0.00190
## 57  2 29.75 0.00173
## 58  2 29.75 0.00158
## 59  2 29.75 0.00144
## 60  2 29.75 0.00131
}

The holdout data can be predicted for both hard class predictions and
probabilities. We’ll bind these together into one tibble:\if{html}{\out{<div class="r">}}\preformatted{bind_cols(
  predict(mr_cls_fit, penguins_test),
  predict(mr_cls_fit, penguins_test, type = "prob")
)
}\if{html}{\out{</div>}}\preformatted{## # A tibble: 5 x 4
##   .pred_class .pred_Biscoe .pred_Dream .pred_Torgersen
##   <fct>              <dbl>       <dbl>           <dbl>
## 1 Dream              0.339      0.448           0.214 
## 2 Biscoe             0.879      0.0882          0.0331
## 3 Biscoe             0.539      0.317           0.144 
## 4 Dream              0.403      0.435           0.162 
## 5 Dream              0.297      0.481           0.221
}
}

}

\subsection{References}{
\itemize{
\item Hastie, T, R Tibshirani, and M Wainwright. 2015. \emph{Statistical
Learning with Sparsity}. CRC Press.
\item Kuhn, M, and K Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer.
}
}
}
\keyword{internal}
