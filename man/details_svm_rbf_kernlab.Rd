% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svm_rbf_kernlab.R
\name{details_svm_rbf_kernlab}
\alias{details_svm_rbf_kernlab}
\title{Radial basis function support vector machines (SVMs) via kernlab}
\description{
\code{\link[kernlab:ksvm]{kernlab::ksvm()}} fits a support vector machine model. For classification,
these models try to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by the largest model residuals.
}
\details{
For this engine, there are multiple modes: classification and regression
\subsection{Tuning Parameters}{

This model has 3 tuning parameters:
\itemize{
\item \code{cost}: Cost (type: double, default: 1.0)
\item \code{rbf_sigma}: Radial Basis Function sigma (type: double, default: see
below)
\item \code{margin}: Insensitivity Margin (type: double, default: 0.1)
}

There is no default for the radial basis function kernel parameter.
\code{kernlab} estimates it from the data using a heuristic method. See
\code{\link[kernlab:sigest]{kernlab::sigest()}}. This method uses random
numbers so, without setting the seed before fitting, the model will not
be reproducible.
}

\subsection{Translation from parsnip to the original package (regression)}{\if{html}{\out{<div class="r">}}\preformatted{svm_rbf(
  cost = double(1),
  rbf_sigma = double(1), 
  margin = double(1)
) \%>\%  
  set_engine("kernlab") \%>\% 
  set_mode("regression") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Radial Basis Function Support Vector Machine Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   rbf_sigma = double(1)
##   margin = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     epsilon = double(1), kernel = "rbfdot", kpar = list(sigma = ~double(1)))
}
}

\subsection{Translation from parsnip to the original package (classification)}{\if{html}{\out{<div class="r">}}\preformatted{svm_rbf(
  cost = double(1),
  rbf_sigma = double(1)
) \%>\% 
  set_engine("kernlab") \%>\% 
  set_mode("classification") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Radial Basis Function Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
##   rbf_sigma = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     kernel = "rbfdot", prob.model = TRUE, kpar = list(sigma = ~double(1)))
}

The \code{margin} parameter does not apply to classification models.

Note that the \code{kernlab} engine does not naturally estimate class
probabilities. To get them, the decision values of the model are
converted to probabilities using Platt scaling. This method fits an
additional model on top of the SVM model. When fitting the Platt scaling
model, random numbers are used that are not reproducible or controlled
by R’s random number stream.
}

\subsection{Preprocessing requirements}{

Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via
\code{\link[=fit.model_spec]{fit.model_spec()}}, \code{parsnip} will
convert factor columns to indicators.

Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
}

\subsection{Working examples}{
\subsection{Regression Example}{

We’ll model the ridership on the Chicago elevated trains as a function
of the 14 day lagged ridership at two stations. The two predictors are
in the same units (rides per day/1000) and do not need to be normalized.

All but the last week of data are used for training. The last week will
be predicted after the model is fit.\if{html}{\out{<div class="r">}}\preformatted{library(tidymodels)
tidymodels_prefer()
data(Chicago)

n <- nrow(Chicago)
Chicago <- Chicago \%>\% select(ridership, Clark_Lake, Quincy_Wells)

Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
}\if{html}{\out{</div>}}

We can define the model with specific parameters:\if{html}{\out{<div class="r">}}\preformatted{svm_reg_spec <- 
  svm_rbf(cost = 1, margin = 0.1) \%>\% 
  # This model can be used for classification or regression, so set mode
  set_mode("regression") \%>\% 
  set_engine("kernlab")
svm_reg_spec
}\if{html}{\out{</div>}}\preformatted{## Radial Basis Function Support Vector Machine Specification (regression)
## 
## Main Arguments:
##   cost = 1
##   margin = 0.1
## 
## Computational engine: kernlab
}

Now we create the model fit object:\if{html}{\out{<div class="r">}}\preformatted{set.seed(1)
svm_reg_fit <- svm_reg_spec \%>\% fit(ridership ~ ., data = Chicago_train)
svm_reg_fit
}\if{html}{\out{</div>}}\preformatted{## parsnip model object
## 
## Fit time:  2.2s 
## Support Vector Machine object of class "ksvm" 
## 
## SV type: eps-svr  (regression) 
##  parameter : epsilon = 0.1  cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  10.8262370251485 
## 
## Number of Support Vectors : 2233 
## 
## Objective Function Value : -746.584 
## Training error : 0.205567
}

The holdout data can be predicted:\if{html}{\out{<div class="r">}}\preformatted{predict(svm_reg_fit, Chicago_test)
}\if{html}{\out{</div>}}\preformatted{## # A tibble: 7 x 1
##   .pred
##   <dbl>
## 1 20.7 
## 2 21.2 
## 3 21.3 
## 4 21.1 
## 5 19.4 
## 6  6.77
## 7  6.13
}
}

\subsection{Classification Example}{

The example data has two predictors and an outcome with two classes.
Both predictors are in the same units\if{html}{\out{<div class="r">}}\preformatted{library(tidymodels)
tidymodels_prefer()
data(two_class_dat)

data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
}\if{html}{\out{</div>}}

We can define the model with specific parameters:\if{html}{\out{<div class="r">}}\preformatted{svm_cls_spec <- 
  svm_rbf(cost = 1) \%>\% 
  # This model can be used for classification or regression, so set mode
  set_mode("classification") \%>\% 
  set_engine("kernlab")
svm_cls_spec
}\if{html}{\out{</div>}}\preformatted{## Radial Basis Function Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = 1
## 
## Computational engine: kernlab
}

Now we create the model fit object:\if{html}{\out{<div class="r">}}\preformatted{set.seed(1)
svm_cls_fit <- svm_cls_spec \%>\% fit(Class ~ ., data = data_train)
svm_cls_fit
}\if{html}{\out{</div>}}\preformatted{## parsnip model object
## 
## Fit time:  97ms 
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  1.63216688499952 
## 
## Number of Support Vectors : 327 
## 
## Objective Function Value : -294.4344 
## Training error : 0.169014 
## Probability model included.
}

The holdout data can be predicted for both hard class predictions and
probabilities. We’ll bind these together into one tibble:\if{html}{\out{<div class="r">}}\preformatted{predict(svm_cls_fit, data_test, type = "prob") \%>\% 
  bind_cols(
    predict(svm_cls_fit, data_test)
  )
}\if{html}{\out{</div>}}\preformatted{## # A tibble: 10 x 3
##    .pred_Class1 .pred_Class2 .pred_class
##           <dbl>        <dbl> <fct>      
##  1        0.238       0.762  Class2     
##  2        0.905       0.0950 Class1     
##  3        0.619       0.381  Class1     
##  4        0.879       0.121  Class1     
##  5        0.641       0.359  Class1     
##  6        0.153       0.847  Class2     
##  7        0.745       0.255  Class1     
##  8        0.313       0.687  Class2     
##  9        0.878       0.122  Class1     
## 10        0.137       0.863  Class2
}
}

}

\subsection{References}{
\itemize{
\item Lin, HT, and R Weng. \href{https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf}{“A Note on Platt’s Probabilistic Outputs for Support Vector Machines”}
\item Karatzoglou, A, Smola, A, Hornik, K, and A Zeileis. 2004.
\href{https://www.jstatsoft.org/article/view/v011i09}{“kernlab - An S4 Package for Kernel Methods in R.”}, \emph{Journal of
Statistical Software}.
\item Kuhn, M, and K Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer.
}
}
}
\keyword{internal}
