% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/svm_poly_kernlab.R
\name{details_svm_poly_kernlab}
\alias{details_svm_poly_kernlab}
\title{Polynomial support vector machines (SVMs) via kernlab}
\description{
\code{\link[kernlab:ksvm]{kernlab::ksvm()}} fits a support vector machine model. For classification,
the model tries to maximize the width of the margin between classes.
For regression, the model optimizes a robust loss function that is only
affected by very large model residuals.
}
\details{
For this engine, there are multiple modes: classification and regression
\subsection{Tuning Parameters}{

This model has 4 tuning parameters:
\itemize{
\item \code{cost}: Cost (type: double, default: 1.0)
\item \code{degree}: Degree of Interaction (type: integer, default: 1L1)
\item \code{scale_factor}: Scale Factor (type: double, default: 1.0)
\item \code{margin}: Insensitivity Margin (type: double, default: 0.1)
}
}

\subsection{Translation from parsnip to the original package (regression)}{\if{html}{\out{<div class="r">}}\preformatted{svm_poly(
  cost = double(1),
  degree = integer(1),
  scale_factor = double(1), 
  margin = double(1)
) \%>\%  
  set_engine("kernlab") \%>\% 
  set_mode("regression") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Polynomial Support Vector Machine Specification (regression)
## 
## Main Arguments:
##   cost = double(1)
##   degree = integer(1)
##   scale_factor = double(1)
##   margin = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     epsilon = double(1), kernel = "polydot", kpar = list(degree = ~integer(1), 
##         scale = ~double(1)))
}
}

\subsection{Translation from parsnip to the original package (classification)}{\if{html}{\out{<div class="r">}}\preformatted{svm_poly(
  cost = double(1),
  degree = integer(1),
  scale_factor = double(1)
) \%>\% 
  set_engine("kernlab") \%>\% 
  set_mode("classification") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Polynomial Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = double(1)
##   degree = integer(1)
##   scale_factor = double(1)
## 
## Computational engine: kernlab 
## 
## Model fit template:
## kernlab::ksvm(x = missing_arg(), data = missing_arg(), C = double(1), 
##     kernel = "polydot", prob.model = TRUE, kpar = list(degree = ~integer(1), 
##         scale = ~double(1)))
}

The \code{margin} parameter does not apply to classification models.

Note that the \code{"kernlab"} engine does not naturally estimate class
probabilities. To produce them, the decision values of the model are
converted to probabilities using Platt scaling. This method fits an
additional model on top of the SVM model. When fitting the Platt scaling
model, random numbers are used that are not reproducible or controlled
by R’s random number stream.
}

\subsection{Preprocessing requirements}{

Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via
\code{\link[=fit.model_spec]{fit.model_spec()}}, parsnip will
convert factor columns to indicators.

Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
}

\subsection{Examples}{
\subsection{Regression Example}{

We’ll model the ridership on the Chicago elevated trains as a function
of the 14 day lagged ridership at two stations. The two predictors are
in the same units (rides per day/1000) and do not need to be normalized.

All but the last week of data are used for training. The last week will
be predicted after the model is fit.\if{html}{\out{<div class="r">}}\preformatted{library(tidymodels)
tidymodels_prefer()
data(Chicago)

n <- nrow(Chicago)
Chicago <- Chicago \%>\% select(ridership, Clark_Lake, Quincy_Wells)

Chicago_train <- Chicago[1:(n - 7), ]
Chicago_test <- Chicago[(n - 6):n, ]
}\if{html}{\out{</div>}}

We can define the model with specific parameters:\if{html}{\out{<div class="r">}}\preformatted{svm_reg_spec <- 
  svm_poly(cost = 1, margin = 0.1) \%>\% 
  # This model can be used for classification or regression, so set mode
  set_mode("regression") \%>\% 
  set_engine("kernlab")
svm_reg_spec
}\if{html}{\out{</div>}}\preformatted{## Polynomial Support Vector Machine Specification (regression)
## 
## Main Arguments:
##   cost = 1
##   margin = 0.1
## 
## Computational engine: kernlab
}

Now we create the model fit object:\if{html}{\out{<div class="r">}}\preformatted{set.seed(1)
svm_reg_fit <- svm_reg_spec \%>\% fit(ridership ~ ., data = Chicago_train)
}\if{html}{\out{</div>}}\preformatted{##  Setting default kernel parameters
}\if{html}{\out{<div class="r">}}\preformatted{svm_reg_fit
}\if{html}{\out{</div>}}\preformatted{## parsnip model object
## 
## Fit time:  598ms 
## Support Vector Machine object of class "ksvm" 
## 
## SV type: eps-svr  (regression) 
##  parameter : epsilon = 0.1  cost C = 1 
## 
## Polynomial kernel function. 
##  Hyperparameters : degree =  1  scale =  1  offset =  1 
## 
## Number of Support Vectors : 2283 
## 
## Objective Function Value : -825.1628 
## Training error : 0.226471
}

The holdout data can be predicted:\if{html}{\out{<div class="r">}}\preformatted{predict(svm_reg_fit, Chicago_test)
}\if{html}{\out{</div>}}\preformatted{## # A tibble: 7 x 1
##   .pred
##   <dbl>
## 1 21.0 
## 2 21.2 
## 3 21.5 
## 4 21.2 
## 5 19.4 
## 6  6.87
## 7  6.41
}
}

\subsection{Classification Example}{

The example data has two predictors and an outcome with two classes.
Both predictors are in the same units\if{html}{\out{<div class="r">}}\preformatted{library(tidymodels)
tidymodels_prefer()
data(two_class_dat)

data_train <- two_class_dat[-(1:10), ]
data_test  <- two_class_dat[  1:10 , ]
}\if{html}{\out{</div>}}

We can define the model with specific parameters:\if{html}{\out{<div class="r">}}\preformatted{svm_cls_spec <- 
  svm_poly(cost = 1) \%>\% 
  # This model can be used for classification or regression, so set mode
  set_mode("classification") \%>\% 
  set_engine("kernlab")
svm_cls_spec
}\if{html}{\out{</div>}}\preformatted{## Polynomial Support Vector Machine Specification (classification)
## 
## Main Arguments:
##   cost = 1
## 
## Computational engine: kernlab
}

Now we create the model fit object:\if{html}{\out{<div class="r">}}\preformatted{set.seed(1)
svm_cls_fit <- svm_cls_spec \%>\% fit(Class ~ ., data = data_train)
}\if{html}{\out{</div>}}\preformatted{##  Setting default kernel parameters
}\if{html}{\out{<div class="r">}}\preformatted{svm_cls_fit
}\if{html}{\out{</div>}}\preformatted{## parsnip model object
## 
## Fit time:  38ms 
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Polynomial kernel function. 
##  Hyperparameters : degree =  1  scale =  1  offset =  1 
## 
## Number of Support Vectors : 353 
## 
## Objective Function Value : -349.425 
## Training error : 0.174136 
## Probability model included.
}

The holdout data can be predicted for both hard class predictions and
probabilities. We’ll bind these together into one tibble:\if{html}{\out{<div class="r">}}\preformatted{bind_cols(
  predict(svm_cls_fit, data_test),
  predict(svm_cls_fit, data_test, type = "prob")
)
}\if{html}{\out{</div>}}\preformatted{## # A tibble: 10 x 3
##    .pred_class .pred_Class1 .pred_Class2
##    <fct>              <dbl>        <dbl>
##  1 Class1             0.517      0.483  
##  2 Class1             0.904      0.0956 
##  3 Class1             0.645      0.355  
##  4 Class1             0.610      0.390  
##  5 Class2             0.445      0.555  
##  6 Class2             0.212      0.788  
##  7 Class1             0.704      0.296  
##  8 Class1             0.565      0.435  
##  9 Class1             0.994      0.00646
## 10 Class2             0.114      0.886
}
}

}

\subsection{References}{
\itemize{
\item Lin, HT, and R Weng. \href{https://www.csie.ntu.edu.tw/~cjlin/papers/plattprob.pdf}{“A Note on Platt’s Probabilistic Outputs for Support Vector Machines”}
\item Karatzoglou, A, Smola, A, Hornik, K, and A Zeileis. 2004.
\href{https://www.jstatsoft.org/article/view/v011i09}{“kernlab - An S4 Package for Kernel Methods in R.”}, \emph{Journal of
Statistical Software}.
\item Kuhn, M, and K Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer.
}
}
}
\keyword{internal}
