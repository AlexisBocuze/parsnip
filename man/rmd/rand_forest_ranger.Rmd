```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("rand_forest", "ranger")`

## Tuning Parameters

```{r ranger-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("mtry", "trees", "min_n"),
                 default = c("see below", "500L", "see below"))

param <-
  rand_forest() %>% 
  set_engine("ranger") %>% 
  tunable() %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r ranger-param-list, echo = FALSE, results = "asis"}
param$item
```

`mtry` depends on the number of columns. The default in [ranger::ranger()] is `floor(sqrt(ncol(x)))`.

`min_n` depends on the mode. For regression, a value of 5 is the default. For classification, a value of 10 is used. 

## Translation from parsnip to the original package (regression)

```{r ranger-reg}
rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %>%  
  set_engine("ranger") %>% 
  set_mode("regression") %>% 
  translate()
```

`min_rows()` and `min_cols()` will adjust the number of neighbors if the chosen value if it is not consistent with the actual data dimensions.

## Translation from parsnip to the original package (classification)

```{r ranger-cls}
rand_forest(
  mtry = integer(1),
  trees = integer(1),
  min_n = integer(1)
) %>% 
  set_engine("ranger") %>% 
  set_mode("classification") %>% 
  translate()
```

Note that a `ranger` probability forest is always fit (unless the `probability` argument is changed by the user via [set_engine()]).

## Preprocessing requirements

```{r child = "template-split-factors.Rmd"}
```

## Other notes

By default, parallel processing is turned off. To run each model in parallel, change the `num.threads` argument via [set_engine()]. When tuning, it is more efficient to parallelize over the resamples and tuning parameters.

For `ranger` confidence intervals, the intervals are  constructed using the form `estimate +/- z * std_error`. For  classification probabilities, these values can fall outside of  `[0, 1]` and will be coerced to be in this range.

## Working examples

```{r child = "template-reg-chicago.Rmd"}
```

We can define the model with specific parameters:

```{r}
rf_reg_spec <- 
  rand_forest(trees = 200, min_n = 5) %>% 
  # This model can be used for classification or regression, so set mode
  set_mode("regression") %>% 
  set_engine("ranger")
rf_reg_spec
```

Now we create the model fit object:

```{r}
set.seed(1)
rf_reg_fit <- rf_reg_spec %>% fit(ridership ~ ., data = Chicago_train)
rf_reg_fit
```

The holdout data can be predicted:

```{r}
predict(rf_reg_fit, Chicago_test)
```


```{r child = "template-cls-two-class.Rmd"}
```

```{r}
rf_cls_spec <- 
  rand_forest(trees = 200, min_n = 5) %>% 
  # This model can be used for classification or regression, so set mode
  set_mode("classification") %>% 
  set_engine("ranger")
rf_cls_spec
```

Now we create the model fit object:

```{r}
set.seed(1)
rf_cls_fit <- rf_cls_spec %>% fit(Class ~ ., data = data_train)
rf_cls_fit
```

The holdout data can be predicted for both hard class predictions and probabilities. We'll bind these together into one tibble:

```{r}
predict(rf_cls_fit, data_test, type = "prob") %>% 
  bind_cols(
    predict(rf_cls_fit, data_test)
  )
```

## References

 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.
