```{r, child = "setup.Rmd", include = FALSE}
```

`r descr_models("boost_tree", "spark")`. However, multiclass classification is not supported yet.

## Tuning Parameters

```{r spark-param-info, echo = FALSE}
defaults <- 
  tibble::tibble(parsnip = c("tree_depth", "trees", "learn_rate", "mtry", "min_n", "loss_reduction", "sample_size"),
                 default = c("5L", "20L", "0.1", "see below", "1L", "0.0", "1.0"))

# For this model, this is the same for all modes
param <-
 boost_tree() %>% 
  set_engine("spark") %>% 
  set_mode("regression") %>% 
  tunable() %>% 
  dplyr::filter(name != "stop_iter") %>% 
  dplyr::select(-source, -component, -component_id, parsnip = name) %>% 
  dplyr::mutate(
    dials = purrr::map(call_info, get_dials),
    label = purrr::map_chr(dials, ~ .x$label),
    type = purrr::map_chr(dials, ~ .x$type)
  ) %>% 
  dplyr::full_join(defaults, by = "parsnip") %>% 
  mutate(
    item = 
      glue::glue("- `{parsnip}`: {label} (type: {type}, default: {default})\n\n")
  )
```

This model has `r nrow(param)` tuning parameters:

```{r spark-param-list, echo = FALSE, results = "asis"}
param$item
```

The `mtry` parameter is related to the number of predictors. The default depends on the model mode. For classification, the square root of the number of predictors is used and for regression one third of the predictors are sampled. 

## Translation from parsnip to the original package (regression)

```{r spark-reg}
boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric()
) %>%
  set_engine("spark") %>%
  set_mode("regression") %>%
  translate()
```

## Translation from parsnip to the original package (classification)

```{r spark-cls}
boost_tree(
  mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
  learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric()
) %>% 
  set_engine("spark") %>% 
  set_mode("classification") %>% 
  translate()
```

## Preprocessing requirements

`spark` does not require any special encoding of the predictors. Categorical predictors can be split into groups without creating dummy variables for each category.

## Other details

For models created using the `spark` engine, there are several differences to consider. 

* Only the formula interface to via `fit()` is available; using `fit_xy()` will generate an error. 
* The predictions will always be in a spark table format. The names will be the same as documented but without the dots. 
* There is no equivalent to factor columns in spark tables so class predictions are returned as character columns. 
* To retain the model object for a new R session (via `save()`), the `model$fit` element of the parsnip object should be serialized via `ml_save(object$fit)` and separately saved to disk. In a new session, the object can be reloaded and reattached to the parsnip object.

## References

 - Kuhn, M, and K Johnson. 2013. _Applied Predictive Modeling_. Springer.

